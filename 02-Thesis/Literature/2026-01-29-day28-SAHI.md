#day28 SAHI: A Generalized Framework for Small Object Detection

今天读SAHI《SAHI: A GENERALIZED FRAMEWORK FOR SMALL OBJECT DETECTION》（2022，商汤科技等，Naci Dikici等）。

Day27 QueryDet用coarse-to-fine策略聚焦小目标，今天SAHI说：更简单的方法——把图切成小块，一块一块检测，再合并结果。

核心痛点：小目标检测的"尺寸失配"问题。

现代检测器大多在ImageNet上预训练，输入尺寸通常是640×640或800×1333。但当图像中有很多小目标时，需要更高分辨率才能看清。直接提高输入分辨率？计算量爆炸，显存不够。

作者的解法：SAHI = Slicing Aided Hyper Inference。

核心思想：不用整图高分辨率推理，而是把图像切成多个重叠的小patch（如256×256），在每个patch上独立检测，最后合并所有检测结果。

三步流程：
1. Slicing：将图像切成重叠的小块（overlap如25%）
2. Inference：在每个patch上独立运行检测器
3. Merging：合并所有patch的检测结果，用NMS去重

关键创新：切片重叠解决了"边界目标被截断"的问题——一个跨边界的目标会在两个相邻patch中被检测到，合并时通过NMS保留一个。

关键发现：简单的切片推理，小目标大幅提升。

COCO test-dev 2017 - 小目标（Table 2）：
SAHI + Faster R-CNN R-101：29.8% APs
Baseline Faster R-CNN：24.2% APs（+5.6%）

VisDrone数据集（Table 3）：
SAHI + YOLOv5l：39.8% mAP
Baseline YOLOv5l：33.0% mAP（+6.8%）

表4显示：切片尺寸256×256效果最好，overlap 25%比0%提升约1 AP。

Table 5的消融实验显示：切片+合并比直接高分辨率推理（2×scale）更快，且精度相当。

读后感：
Day26 TPH-YOLOv5修改网络结构，Day27 QueryDet修改训练策略，今天SAHI说"都不用改，只改推理方式就行"。SAHI的优雅之处在于模型无关——任何检测器都可以用，不需要重新训练。把大图切成小块，让检测器在它"熟悉"的尺度上工作，小目标就变大了。这种"分而治之"的思想后来成为小目标检测的标准做法，尤其是遥感图像、医学图像等高分辨率场景。
