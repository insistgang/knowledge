#day24 SAM: Segment Anything

今天读SAM《SEGMENT ANYTHING》（ICCV 2023，Meta AI，Alexander Kirillov等）。

Day23 EVA-02刷新ViT精度SOTA，今天SAM说：分割任务也需要foundation model——而且要能响应"任何提示"。

核心痛点：分割任务的"碎片化"困境。

传统的分割模型很"专"：交互式分割、语义分割、实例分割、全景分割……每个任务都要单独训练模型。而且需要大量标注数据，换到新场景就得重新标注。

作者的解法：SAM = Promptable Segmentation Foundation Model。

核心思想：把分割变成"prompt → mask"的通用任务。

SAM支持三种prompt输入：
1. 点：前景点/背景点
2. 框：包围盒
3. Mask：粗略mask作为引导

模型架构：
Image Encoder：ViT-H/16（MAE预训练）
Prompt Encoder：编码各种prompt
Mask Decoder：预测mask及其置信度

关键创新：Ambiguity-aware——当一个prompt对应多个合理mask时（比如点在衬衫上，是分割衬衫还是分割身体？），SAM会输出多个预测，让用户选择。

关键发现：11亿mask训练出的分割foundation model，zero-shot吊打专门模型。

SA-1B数据集（Table 1）：
11M 高分辨率图像
1.1B 高质量分割mask
平均每张图约100个mask

Zero-shot transfer到23个分割数据集（Table 3）：
SAM (ViT-H)：52.0 NC（归一化mIoU）
专门训练的RITM：39.2 NC（+12.8）

COCO边框检测（Table 5，用mask生成bbox）：
SAM：62.3 AP
Mask R-CNN：50.6 AP（+11.7）

Table 4显示：SAM在ADE20K分割任务上也能达到43.4 mIoU，接近专门训练的Segment-Anything-Mask R-CNN的42.5。

读后感：
Day06 ViT证明了Transformer能做图像分类，今天SAM证明了Transformer能做"万物分割"。SAM的意义不仅在于精度，更在于通用性——一个模型处理所有分割任务，响应任何形式的prompt。这种"promptable"的设计后来被Segment Anything 2、SAM 2等工作继承，也影响了后续的视觉foundation model设计。SAM让分割从"专门任务"变成了"通用能力"。
