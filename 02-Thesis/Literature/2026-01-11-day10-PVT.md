#day10 Pyramid Vision Transformer (PVT)

今天读PVT《Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions》（ICCV 2021，华中科大&华为诺亚，Wenhai Wang等）。

Day06 ViT把Transformer带进CV，但它是为图像分类设计的——单一分辨率输出，没法做检测分割。Day09 Swin用shifted window解决这个问题。今天PVT走另一条路：用SRA（Spatial Reduction Attention）降低复杂度，构建Transformer版的FPN。

核心痛点：ViT的O(n²)复杂度让高分辨率图像成为奢望，而且非分层结构难以输出多尺度特征。

做检测分割需要类似FPN的特征金字塔（P2/P3/P4/P5），但ViT只有一个输出。Swin用window解决复杂度问题，PVT选择了不同路线。

作者的解法：Pyramid Vision Transformer —— 直接在Transformer里实现金字塔结构。

PVT有4个stage，像ResNet一样逐层下采样（H/4 → H/8 → H/16 → H/32）。核心创新是SRA（Spatial Reduction Attention）：

用stride卷积降低Key/Value的分辨率，Query保持原分辨率
复杂度从O(n²)降到O(n²/s²)，s是reduction ratio
每个stage渐进式降低特征图尺寸，增加通道数

关键发现：第一款"通用Transformer backbone"。

在检测任务上（RetinaNet + PVT-Tiny）：
COCO AP：36.8，超过ResNet-50的35.4
输入分辨率从512×512涨到800×800时，FLOPs只有7.6G，远低于ViT-B的19.8G

在分割任务上（Semantic FPN + PVT-Tiny）：
ADE20K mIoU：36.2，接近ResNet-101的37.4

Table 5显示：随着输入分辨率增加，PVT的优势越明显——这正是密集预测任务的核心需求。

读后感：
Day09 Swin说"把attention限制在window里"，今天PVT说"把KV降采样就行了"。两种思路殊途同归：都让Transformer能处理高分辨率。PVT更简洁，不需要复杂的shift操作；但Swin的归纳偏置让它训练更稳定。后来PVTv2等改进版相继出现，Transformer backbone的路越走越宽。
