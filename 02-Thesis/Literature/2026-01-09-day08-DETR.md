#day08 DETR: End-to-End Object Detection with Transformers

今天读DETR《END-TO-END OBJECT DETECTION WITH TRANSFORMERS》（ECCV 2020，FAIR，Nicolas Carion等）。

Day03 Transformer革命了NLP，Day06 ViT把Transformer带到图像分类，Day07 FPN优化了特征金字塔。今天DETR说：目标检测也不需要anchor了，直接Transformer端到端干。

核心痛点：目标检测的"人工痕迹"太重。

Faster R-CNN需要设计anchor（9种尺度+比例）、NMS去重、一堆手工设计的超参。检测器更像是在调参，而不是在学习。

作者的解法：把检测变成集合预测问题。

DETR架构分三步走：
1. CNN backbone（如ResNet-50）提取特征
2. Transformer Encoder-Decoder处理
3. FFN预测类别和bbox

核心大招：Hungarian Loss（二分匹配）。

让100个可学习的object query通过Transformer decoder，每个query预测一个bbox。训练时用匈牙利算法找到GT和预测的最优匹配（一对一），然后算loss。不需要NMS，因为天然就是一一对应的。

关键发现：简单，但慢。

COCO上DETR-R50达到42.0 AP，对标Faster R-CNN-FPN。但有个致命问题：收敛慢——Faster R-CNN用3x schedule（约36 epochs）就能达到不错性能，DETR需要300 epochs，500 epochs才能和增强版baseline持平。

Table 1显示小目标性能差距明显：APs 20.5，而Faster R-CNN-FPN是24.2。全局注意力对高分辨率特征图太贵了。

读后感：
Day03的Transformer说"Attention Is All You Need"，今天DETR说"NMS和Anchor Are All You Don't Need"。但代价是训练时间和小目标检测——这些痛点催生了后续的Deformable DETR、DAB-DETR等工作。DETR的伟大在于：它证明了检测可以端到端，把检测从"工程问题"变成了纯粹的"学习问题"。
