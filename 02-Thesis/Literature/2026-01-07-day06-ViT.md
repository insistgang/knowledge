#day06 Vision Transformer (ViT)

今天读Vision Transformer（ViT）《AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》（ICLR 2021，Google Research）。

Day04读ResNet的"+x"，Day05读DenseNet的"[x0,x1,…]"，都在优化CNN的连接方式。今天ViT直接说：不需要CNN了。

核心痛点： CNN的归纳偏置（局部性+平移等变性）是小数据时代的智慧。但当数据量足够大时，这些"人为设计"反而成了天花板。

作者的解法： 直接把图像当文本处理——Patch Embedding。

把图片切成16×16的小方块，每个方块展平就是一个"token"，加[class] token和位置编码后，扔进Transformer Encoder。跟BERT一模一样。

关键发现： 大规模训练打败归纳偏置。

ViT在JFT-300M（3亿图片）上预训练后，ImageNet-1k达到88.55% top-1，干翻ResNet。但如果直接在ImageNet-1k从头训练，效果不如ResNet——数据不够时，CNN的先验知识还是管用的。

读后感：
Day03的Transformer把NLP从RNN时代带进Attention时代，今天的ViT把CV从CNN时代带进Attention时代。图像切块当token，这个idea简单到让人震惊——为什么没人早点想到？答案可能是：算力不够，数据也不够。ViT证明了：当数据够多时，让模型自己学，比人类设计卷积核更有效。
