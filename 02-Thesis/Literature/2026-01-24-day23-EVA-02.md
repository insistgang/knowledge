#day23 EVA-02: A Strong Vision Transformer

今天读EVA-02《EVA-02: A STRONG VISION TRANSFORMER FOR THE MILLION-MODEL ERA》（北京通用AI研究院&港科大&北大，Yizhao Li等）。

Day22 InternImage用动态卷积让CNN学Transformer，今天EVA-02说：ViT的训练也需要进化——一个强teacher就够了，不需要精心设计一堆teacher。

核心痛点：Vision Foundation Model时代的"teacher焦虑"。

EVA-01（同一团队的前作）用精心设计的多个teacher来蒸馏ViT，效果很好但太繁琐。每个teacher负责不同的语义层次，需要人工设计。在VFM时代，这种方式难以扩展。

作者的解法：RDI + Pseudo-Semantic Token + 单一CLIP teacher。

核心机制：

1. RDI（Random & Distilled Initialization）：位置编码不再用固定的sin/cos，而是从teacher那里蒸馏过来，或者干脆随机初始化。

2. Pseudo-Semantic Token：最让人惊讶的发现——用随机初始化的token，或者用CNN生成的"伪语义token"，效果和精心设计的语义token差不多。这说明ViT的patch embedding本身就很强，不需要复杂的token设计。

3. 只用CLIP一个teacher：不需要一堆精心设计的teacher，CLIP一个就够了。

关键发现："一teacher足矣"，EVA-02全面刷新SOTA。

ImageNet-1K top-1（Table 1）：
EVA-02-B：86.9%，超越InternImage-B的85.3%
EVA-02-L：88.3%，超越InternImage-L的87.1%
EVA-02-g：88.8%
EVA-02-e：89.6%

COCO检测（Mask R-CNN，Table 2）：
EVA-02-L：57.4 box AP，50.1 mask AP
对比InternImage-L：56.2 box AP（+1.2）

ADE20K分割（UPP Head，Table 3）：
EVA-02-L：54.8 mIoU
对比InternImage-L：53.6 mIoU（+1.2）

Table 4显示：随机初始化和CNN生成的伪语义token效果相当，都优于固定sin/cos位置编码。这证明ViT的自注意力机制本身足够强大。

读后感：
Day19 DeiT用CNN teacher教ViT student，开启了ViT蒸馏时代。EVA-01说要精心设计多个teacher，今天EVA-02说"一个CLIP就够了"。这说明在VFM时代，teacher的质量比数量更重要——一个在大规模图文对上预训练的CLIP，比一堆精心设计的小teacher更有效。EVA-02也证明了ViT的patch embedding本身就很强，不需要复杂的token设计。
