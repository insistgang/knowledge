#day19 DeiT: Training Data-Efficient Image Transformers

今天读DeiT《TRAINING DATA-EFFICIENT IMAGE TRANSFORMERS & DISTILLATION THROUGH ATTENTION》（ICCV 2021，FAIR，Hugo Touvron等）。

Day06 ViT把Transformer带进图像分类，但需要JFT-300M（3亿图片）预训练才能达到SOTA。今天DeiT说：只用ImageNet-1K（120万图片），就能训练出更好的ViT。

核心痛点：ViT的数据饥渴症。

原始ViT（Google）在JFT-300M上预训练后，ImageNet top-1达到81.8%。但JFT-300M是Google私有的3亿图像数据集，普通研究者根本用不起。如果只在ImageNet-1K上训练ViT，效果很差（约72%），还不如ResNet-50的79.8%。

ViT的"数据饥渴"让很多研究者望而却步。

作者的解法：Knowledge Distillation + Distillation Token。

核心机制：用一个CNN teacher教ViT student。

DeiT引入了"distillation token"：类似class token，是一个可学习的向量，与class token一起通过Transformer。它的作用是专门接收teacher（CNN）的知识。

训练时有两个target：
1. 真实标签（hard label）
2. Teacher的预测（soft label，通过distillation token接收）

推理时：丢弃distillation token，只用class token输出。

关键发现：用CNN teacher，ViT打败CNN。

ImageNet-1K top-1（Table 2）：
DeiT-B/16：82.3%（仅在ImageNet上训练）
DeiT-B/16 with distillation：83.6%

对比：
ResNet-50：79.8%
ResNet-152：82.4%
ViT-B/16（JFT预训练）：81.8%
DeiT-B/16：82.3%（超越原始ViT，不需要JFT）

Table 3显示：DeiT训练时间只需要3天（8块GPU），而原始ViT在JFT-300M上预训练需要约15天。

读后感：
Day06 ViT证明了Transformer能做图像分类，但需要海量数据。今天DeiT用knowledge distillation解决了数据依赖问题——用成熟的CNN teacher教年轻的ViT student。这种"老带新"的思路后来被广泛采用：用teacher-student训练大模型，再蒸馏成小模型。DeiT让ViT从"奢侈品"变成了"日用品"。
